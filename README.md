## AlphaZero-Go
This is an implementation of the AlphaZero algorithm for playing 5*5 go game from pure self-play training. The go game here is specially designed to simplify this problem, so that we can focus on the training scheme of AlphaZero and obtain a pretty good AI model on a PC in a few hours. 

References:  
1. AlphaZero: Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm
2. AlphaGo Zero: Mastering the game of Go without human knowledge

### Requirements
To run the deployed AI models, only need:
- Python >= 3.7
- Numpy >= 1.20

To train the whole model from scratch, further need:
- PyTorch >= 0.2.0

### File Structure
```
  ├── Training: Training files with pytorch
  │    ├── convert_param.py: Convert torch weight to numpy weight
  │    ├── game.py: Game rules and board paramaters
  │    ├── human_play.py: Test model with human player
  │    ├── mcts_alphaZero.py: Markov tree with alpha zero algorithm
  │    ├── mcts_pure.py: Pure markov tree
  │    ├── policy_value_net_numpy.py: Policy value net written in numpy
  │    ├── policy_value_net_pytorch.py: Policy value net written in pytorch
  │    └── train.py: Training program for this model
  │ 
  ├── Deploying: Deployed model
  │    ├── best_model_weight_numpy.model: Best model paramaters used in my_player.py
  │    ├── build.sh: Program run script
  │    ├── clean.py: Empty the cresult.txt file generated by the last run
  │    ├── host.py: Game host file, host the chessboard where both sides of the battle are located
  │    ├── my_player.py: Go game strategy file
  │    ├── random_player.py: A simple random chess program
  │    ├── read.py: Read file function, which can be used as a tool for reading .txt files
  │    └── write.py: Write file function, same as above
  │ 
  ├── Game_Rule: Explaination of modified go game
  └── Game_Rule_Chinese_Version: Chinese Version of explaination of modified go game
```

### Getting Started
To run a deployed model, run the following script with Git Bash from '/Validing' directory:  
```
sh build.sh
```
Modify the 'play_time' variable in line 96 of the build.sh file to change the number of battles, and modify the random_player in line 36 of 'ta_agent=("random_player")' to another file script name to change the program that you play against

To play with provided models, run the following script from '/Training' directory:  
```
python human_play.py  
```
You may modify human_play.py to try different provided models or the pure MCTS.

To train the AI model from scratch with PyTorch, run the following script from '/Training' directory:  
```
python train.py
```
The models (best_policy.model and current_policy.model) will be saved every a few updates (default 50).  

### Some Explainations
my_player.py doesn't use any off-the-shelf algorithms, all algorithms in my_player.py are build from scratch, so only few packages are used.

The following five packages are all build-in packages in Python:
- import random                    #Usage: give randomness to policy
- import time                      #Usage: count time to avoid time out
- import pickle                    #Usage: convert file into a byte stream for file i/o
- import copy                      #Usage: use deepcopy to copy variables
- import os                        #Usage: delete files

#The following package is additional package:
- import numpy as np               #Usage: mathematical computation

#The following files are dependencies:
- from read import readInput       #Usage: get input
- from write import writeOutput    #Usage: get output